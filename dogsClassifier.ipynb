{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import matplotlib.image as img\n",
    "import PIL.Image\n",
    "import glob\n",
    "import random\n",
    "import base64\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thumbnail(path): \n",
    "    image = img.imread(path)\n",
    "    return image\n",
    "\n",
    "def image_base64(im):\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2184, 256, 256)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs = pd.read_csv('data/filtered_labels.csv')\n",
    "#dogs = dogs[:5000]\n",
    "dogs['file'] = dogs.id.map(lambda id: f'../dogs_data/greyscale/{id}.jpg')\n",
    "\n",
    "imgs = []\n",
    "for f in dogs['file'].values:\n",
    "    imgs.append(get_thumbnail(f))\n",
    "imgs = np.array(imgs)\n",
    "\n",
    "float_imgs = imgs.astype(float)\n",
    "float_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2184, 256, 256, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_imgs = float_imgs.reshape(2184, 256, 256, 1)\n",
    "float_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs.breed = pd.factorize(dogs.breed)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = float_imgs\n",
    "\n",
    "train_labels = dogs['breed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp74xhr01k\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp74xhr01k', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1d95443890>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "def create_network(features, labels, mode):\n",
    "\n",
    "    i = tf.reshape(features['x'], [-1, 256, 256, 1])\n",
    "    \n",
    "    # receives [batch_size, 256, 256, 3]\n",
    "    # returns [batch_size, 256, 256, 32]\n",
    "    convolution1 = tf.layers.conv2d(inputs = i, filters = 32, kernel_size = [5,5], activation = tf.nn.relu,\n",
    "                                 padding = 'same')\n",
    "    \n",
    "    # receives [batch_size, 256, 256, 32]\n",
    "    # returns [batch_size, 128, 128, 32]\n",
    "    pooling1 = tf.layers.max_pooling2d(inputs = convolution1, pool_size = [2,2], strides = 2)\n",
    "    \n",
    "    # receives [batch_size, 128, 128, 32]\n",
    "    # returns [batch_size, 128, 128, 64]\n",
    "    convolution2 = tf.layers.conv2d(inputs = pooling1, filters = 64, kernel_size = [5,5], activation = tf.nn.relu,\n",
    "                                  padding = 'same')\n",
    "    \n",
    "    # receives [batch_size, 128, 128, 64]\n",
    "    # returns [batch_size, 64, 64, 64]\n",
    "    pooling2 = tf.layers.max_pooling2d(inputs = convolution2, pool_size = [2,2], strides = 2)\n",
    "    \n",
    "    # receives [batch_size, 64, 64, 64]\n",
    "    # returns [batch_size, 64, 64, 128]\n",
    "    convolution3 = tf.layers.conv2d(inputs = pooling2, filters = 128, kernel_size = [5,5], activation = tf.nn.relu,\n",
    "                                 padding = 'same')\n",
    "    \n",
    "    # receives [batch_size, 64, 64, 128]\n",
    "    # returns [batch_size, 32, 32, 128]\n",
    "    pooling3 = tf.layers.max_pooling2d(inputs = convolution3, pool_size = [2,2], strides = 2)\n",
    "    \n",
    "    # receives [batch_size, 32, 32, 128]\n",
    "    # returns [batch_size, 32, 32, 256]\n",
    "    convolution4 = tf.layers.conv2d(inputs = pooling3, filters = 256, kernel_size = [5,5], activation = tf.nn.relu,\n",
    "                                 padding = 'same')\n",
    "    \n",
    "    # receives [batch_size, 32, 32, 256]\n",
    "    # returns [batch_size, 16, 16, 256]\n",
    "    pooling4 = tf.layers.max_pooling2d(inputs = convolution4, pool_size = [2,2], strides = 2)\n",
    "    \n",
    "    # receives [batch_size, 16, 16, 256]\n",
    "    # returns [batch_size, 16, 16, 512]\n",
    "    convolution5 = tf.layers.conv2d(inputs = pooling4, filters = 512, kernel_size = [5,5], activation = tf.nn.relu,\n",
    "                                 padding = 'same')\n",
    "    \n",
    "    # receives [batch_size, 16, 16, 512]\n",
    "    # returns [batch_size, 8, 8, 512]\n",
    "    pooling5 = tf.layers.max_pooling2d(inputs = convolution5, pool_size = [2,2], strides = 2)\n",
    "    \n",
    "    # receives [batch_size, 8, 8, 512]\n",
    "    # returns [batch_size, 8, 8, 1024]\n",
    "    convolution6 = tf.layers.conv2d(inputs = pooling5, filters = 1024, kernel_size = [5,5], activation = tf.nn.relu,\n",
    "                                 padding = 'same')\n",
    "    \n",
    "    # receives [batch_size, 8, 8, 1024]\n",
    "    # returns [batch_size, 4, 4, 1024]\n",
    "    pooling6 = tf.layers.max_pooling2d(inputs = convolution6, pool_size = [2,2], strides = 2)\n",
    "    \n",
    "    # receives [batch_size, 4, 4, 1024]\n",
    "    # returns [batch_size, 4*4*1024]\n",
    "    flattening = tf.reshape(pooling6, [-1, 4 * 4 * 1024])\n",
    "    \n",
    "    # 3136 inputs -> 1024 neurons on hidden layer -> 10 outputs\n",
    "    # receives [batch_size, 3136]\n",
    "    # returns [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs = flattening, units = 1024, activation = tf.nn.relu)\n",
    "    \n",
    "    dense2 = tf.layers.dense(inputs = dense, units = 1024, activation = tf.nn.relu)\n",
    "    \n",
    "    # dropout\n",
    "    dropout =  tf.layers.dropout(inputs = dense2, rate = 0.2, training = mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    # receives [batch_size, 1024]\n",
    "    # returns [batch_size, 10]\n",
    "    output = tf.layers.dense(inputs = dropout, units = 20)\n",
    "    \n",
    "    predictions = tf.argmax(output, axis = 1)\n",
    "    \n",
    "    if(mode == tf.estimator.ModeKeys.PREDICT):\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, predictions = predictions)  \n",
    "    \n",
    "    losses = tf.losses.sparse_softmax_cross_entropy(labels = labels, logits = output)\n",
    "    \n",
    "    if(mode == tf.estimator.ModeKeys.TRAIN):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "        train = optimizer.minimize(losses, global_step = tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, loss = losses, train_op = train)\n",
    "    \n",
    "    if(mode == tf.estimator.ModeKeys.EVAL):\n",
    "        eval_metrics_ops = {'accuracy': tf.metrics.accuracy(labels = labels, predictions = predictions)}\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, loss = losses, eval_metric_ops = eval_metrics_ops) \n",
    "\n",
    "classifier = tf.estimator.Estimator(model_fn = create_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_treinamento, x_teste, y_treinamento, y_teste = train_test_split(train_images, train_labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_function = tf.estimator.inputs.numpy_input_fn(x = {'x': train_images}, y = train_labels, \n",
    "                                                        batch_size= 50, num_epochs= None, shuffle= True)\n",
    "classifier.train(input_fn = train_function, steps = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_function = tf.estimator.inputs.numpy_input_fn(x = {'x': x_test}, y = y_test, num_epochs = 1, shuffle = False)\n",
    "\n",
    "results = classifier.evaluate(input_fn = test_function)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_function = tf.estimator.inputs.numpy_input_fn(x = {'x': x_image_test}, shuffle = False)\n",
    "pred = list(classifier.predict(input_fn = prediction_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "int_row_to_float = lambda row: list(map(lambda element: float(element), row))\n",
    "int_matrix_to_float = lambda matrix: list(map(lambda row: int_row_to_float(row), matrix))\n",
    "img_to_float = lambda img: list(map(lambda dimension: int_matrix_to_float(dimension), img))\n",
    "\n",
    "float_imgs = list(map(lambda img: img_to_float(img), imgs[:5]))\n",
    "float_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dogs['Indexes'] = dogs[\"breed\"].str.startswith('appenzeller')\n",
    "    \n",
    "result = dogs[(dogs.Indexes == True)]\n",
    "result.tail()\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "HTML(result[['breed', 'image']].to_html(formatters={'image': image_formatter}, escape=False))\n",
    "#pd.reset_option('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
